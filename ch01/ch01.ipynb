{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch01.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1LO_W45m60rTlKHz8gArxZ9RS-hxonsat",
      "authorship_tag": "ABX9TyMZ3gTRTyKowjNMLjqX+7RH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kuuumaaaa/data_analyze_technique_in_kaggle/blob/main/ch01/ch01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4lQNt5PfCsY"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fEhZlOChTX9"
      },
      "source": [
        "学習データとテストデータの読み込み\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_xk09xYfIY1"
      },
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/参考書/Kaggleで勝つデータ分析の技術/kagglebook-master/input/ch01-titanic/train.csv')\n",
        "train_x = train.drop(['Survived'],axis =1)\n",
        "train_y = train['Survived']\n",
        "test = pd.read_csv('/content/drive/MyDrive/参考書/Kaggleで勝つデータ分析の技術/kagglebook-master/input/ch01-titanic/test.csv')\n",
        "test_x = test.copy()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6g1Hqt3i4lh"
      },
      "source": [
        "# EDAのポイント\n",
        "カラムの把握、各カラムの方や値の分布、欠損値、外れ値、目的変数と各変数の相関や関係性\n",
        "- 変数の平均・標準偏差・最大・最小・分位点\n",
        "- カテゴリ変数の値の種類数\n",
        "- 変数の欠損値の値\n",
        "- 変数間の相関関係"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "153grtxohdp-"
      },
      "source": [
        "バリデーションデータへの分け方"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqyA38GOgcZq"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
        "tr_idx, va_idx = list(kf.split(train_x))[0]\n",
        "tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
        "tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg9QGhiZIEhe"
      },
      "source": [
        "#特徴量の作成\n",
        "今回GBDTのモデルで作成\n",
        "- IDは予測に寄与しないため削除\n",
        "- Name,Ticket,Cabinについて煩雑な処理が必要なため削除\n",
        "- 文字はlabel encoding\n",
        "- 欠損はそのまま"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrA8WRpVImk_"
      },
      "source": [
        "from  sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 変数PassengerIdを除去\n",
        "train_x = train_x.drop(['PassengerId'],axis=1)\n",
        "test_x = test_x.drop(['PassengerId'],axis=1)\n",
        "\n",
        "# 変数Name,Ticket,Cabiを削除\n",
        "train_x = train_x.drop(['Name','Ticket','Cabin'],axis=1)\n",
        "test_x = test_x.drop(['Name','Ticket','Cabin'],axis=1)\n",
        "\n",
        "# それぞれのカテゴリ変数にlabel encoding\n",
        "for c in ['Sex','Embarked']:\n",
        "  # 学習データに基づいてどう変換するかを定める\n",
        "  le = LabelEncoder()\n",
        "  le.fit(train_x[c].fillna('NA'))\n",
        "\n",
        "  # 学習データ、テストデータを変換する\n",
        "  train_x[c] = le.transform(train_x[c].fillna('NA'))\n",
        "  test_x[c] = le.transform(test_x[c].fillna('NA'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj_0TgUBM-vE"
      },
      "source": [
        "#モデルの作成\n",
        "- GBDTのライブラリの一つであるxgboostを用いてモデルを学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWmbDEWuOGUU"
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "\n",
        "# モデルの作成及び学習データを与えての学習\n",
        "model = XGBClassifier(n_estimators=20, random_state=71)\n",
        "# n_estimators : Number of gradient boosted trees. Equivalent to number of boosting rounds.(決定木の数)\n",
        "# random_state (Optional[Union[numpy.random.RandomState, int]]) –Random number seed.\n",
        "model.fit(train_x,train_y)\n",
        "\n",
        "# テストデータの予測値を確率で出力\n",
        "pred = model.predict_proba(test_x)[:,1]\n",
        "# Predict the probability of each X example being of a given class.\n",
        "\n",
        "# テストデータの予測値を二値に変換する\n",
        "pred_label = np.where(pred > 0.5,1,0)\n",
        "# 0.5以上を1に、それ以下を0にする\n",
        "\n",
        "# 提出用ファイルの作成\n",
        "submission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':pred_label})\n",
        "submission.to_csv('/content/drive/MyDrive/参考書/Kaggleで勝つデータ分析の技術/outputs/ch01/submission_first.csv',index=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU_z5ZQTQ41y"
      },
      "source": [
        "#モデルの評価\n",
        "- バリデーションデータへの予測精度をスコアで表すことで評価\n",
        "- 今回はクロスバリデーションを使用し評価\n",
        "- バリデーションとpublic leaderboardには原因がない限り近い値になる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0-Y4bJ5U3y0",
        "outputId": "085e6d7d-8452-4f36-9282-61e0901f7e31"
      },
      "source": [
        "from sklearn.metrics import log_loss, accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# 各foldのスコアを保存するリスト\n",
        "scores_accuracy = []\n",
        "scores_logloss = []\n",
        "\n",
        "# クロスバリデーションを行う\n",
        "# 学習データを４つに分割し、うち一つをバリデーションデータとすることを、バリデーションデータを変えて繰り返す\n",
        "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
        "for tr_idx, va_idx in kf.split(train_x):\n",
        "  # 学習データを学習データとバリデーションデータに分ける\n",
        "  tr_x,va_x = train_x.iloc[tr_idx],train_x.iloc[va_idx]\n",
        "  tr_y,va_y = train_y.iloc[tr_idx],train_y.iloc[va_idx]\n",
        "\n",
        "  # モデルの学習を行う\n",
        "  model = XGBClassifier(n_estimators=20, random_state=71)\n",
        "  model.fit(tr_x,tr_y)\n",
        "\n",
        "  # バリデーションデータの予測値を確率で出力\n",
        "  va_pred = model.predict_proba(va_x)[:,1]\n",
        "\n",
        "  # バリデーションデータでのスコアを計算する\n",
        "  logloss = log_loss(va_y,va_pred)\n",
        "  accuracy = accuracy_score(va_y,va_pred> 0.5)\n",
        "\n",
        "  # そのfoldのスコアを保存する\n",
        "  scores_logloss.append(logloss)\n",
        "  scores_accuracy.append(accuracy)\n",
        "\n",
        "  # 各foldのスコアの平均を出力する\n",
        "  logloss = np.mean(scores_logloss)\n",
        "  accuracy = np.mean(scores_accuracy)\n",
        "  print(f'logloss:{logloss:.4f},accuracy: {accuracy:4f}')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "logloss:0.3972,accuracy: 0.834081\n",
            "logloss:0.4155,accuracy: 0.820628\n",
            "logloss:0.4265,accuracy: 0.814649\n",
            "logloss:0.4270,accuracy: 0.814815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0aeVzjDnOgp"
      },
      "source": [
        "#モデルのチューニング\n",
        "ハイパーパラメータのチューニング、今回はグリッドサーチを用いる。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSqKN973nniM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dae5dbe-240e-4242-f324-9c04db3e3a7b"
      },
      "source": [
        "import itertools\n",
        "\n",
        "# チューニング候補とするパラメータを準備\n",
        "param_space = {\n",
        "    'max_depth' : [3,5,7],\n",
        "    'min_child_weight' : [1.0, 2.0, 4.0]\n",
        "}\n",
        "\n",
        "# 探索するハイパーパラメータの組み合わせ\n",
        "param_combinations = itertools.product(param_space['max_depth'], param_space['min_child_weight'])\n",
        "\n",
        "# 各パラメータの組み合わせ、それに対するスコアを帆s損するリスト\n",
        "params = []\n",
        "scores = []\n",
        "\n",
        "# 各パラメータの組み合わせごとに、クロスバリデーションで評価を行う\n",
        "for max_depth, min_child_weight in param_combinations:\n",
        "\n",
        "  score_folds = []\n",
        "  # クロスバリデーションを行う\n",
        "  # 学習データを４つに分割し、うち一つをバリデーションデータとすることをバリデーションデータを変えて繰り返す。\n",
        "  kf = KFold(n_splits=4, shuffle=True, random_state=123456)\n",
        "  for tr_idx, va_idx in kf.split(train_x):\n",
        "    # 学習データを学習データとバリデーションデータに分ける\n",
        "    tr_x, va_x = train_x.iloc[tr_idx], train_x.iloc[va_idx]\n",
        "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
        "\n",
        "    # モデルの学習を行う\n",
        "    model = XGBClassifier(n_estimators=20, random_state=71,\n",
        "                          max_depth=max_depth, min_child_weight=min_child_weight)\n",
        "    model.fit(tr_x, tr_y)\n",
        "\n",
        "    # バリデーションデータでのスコアを計算し、保存する\n",
        "    va_pred = model.predict_proba(va_x)[:,1]\n",
        "    logloss = log_loss(va_y,va_pred)\n",
        "    score_folds.append(logloss)\n",
        "    \n",
        "    # 各 foldのスコアを平均する\n",
        "    score_mean = np.mean(score_folds)\n",
        "\n",
        "    # パラメータの組み合わせ、それに対するスコアを保存する\n",
        "    params.append((max_depth, min_child_weight))\n",
        "    scores.append(score_mean)\n",
        "\n",
        "# 最もスコアが良いものをベストなパラメータとする\n",
        "best_idx = np.argsort(scores)[0]\n",
        "best_param = params[best_idx]\n",
        "print(f'maxdepth: {best_param[0]}, min_child_weight: {best_param[1]}')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maxdepth: 7, min_child_weight: 2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3_boVHlq6gR"
      },
      "source": [
        " #アンサンブル\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "b1o2MJJHxIfO",
        "outputId": "a2bcb9b1-7f00-4a4f-d3ab-c142ab5bf8ae"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# xgboostモデル\n",
        "model_xgb  = XGBClassifier(n_estimators=20, random_state=71)\n",
        "model_xgb.fit(train_x, train_y)\n",
        "pred_xgb = model_xgb.predict_proba(test_x)[:,1]\n",
        "\n",
        "# ロジスティック回帰モデル\n",
        "# xgboostモデルとは異なる特徴量を入れる必要があるので、別途train_x2,test_x2を作成\n",
        "model_lr = LogisticRegression(solver='lbfgs' , max_iter=300)\n",
        "model_lr.fit(train_x2,train_y)\n",
        "pred_lr = model_lr.predict_proba(test_x2)[:,1]\n",
        "\n",
        "# 予測値の加重平均をとる\n",
        "pred - pred_xgb * 0.8 + pred_lr * 0.2\n",
        "pred_label = np.where(pred > 0.5 ,1,0)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-ad17cbe71d19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# xgboostモデルとは異なる特徴量を入れる必要があるので、別途train_x2,test_x2を作成\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lbfgs'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mpred_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_x2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiwDwOBRxa5O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}